{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from bert_embedding import BertEmbedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_orig = pd.read_json('../data/annotated_data_full.json')\n",
    "data_orig = pd.read_json('../data/annotated_posts.json')\n",
    "data_orig = data_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>body</th>\n",
       "      <th>id</th>\n",
       "      <th>is_first_post</th>\n",
       "      <th>depth</th>\n",
       "      <th>author</th>\n",
       "      <th>reply_to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>announcement</td>\n",
       "      <td>4/7/13  \\n\\n7/27/12  \\n\\nhttp://www.imdb.com/t...</td>\n",
       "      <td>t3_1bx6qw</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elaboration</td>\n",
       "      <td>I've wanted to watch this for a long time. I w...</td>\n",
       "      <td>t1_c9b2nyd</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>mcgrewf10</td>\n",
       "      <td>t3_1bx6qw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>elaboration</td>\n",
       "      <td>You strike me as the type who would appreciate...</td>\n",
       "      <td>t1_c9b30i1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>t1_c9b2nyd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>elaboration</td>\n",
       "      <td>Yeah, I've always heard that Altman was famous...</td>\n",
       "      <td>t1_c9b6sj0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>mcgrewf10</td>\n",
       "      <td>t1_c9b30i1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>announcement</td>\n",
       "      <td>Alright guys, little background about myself. ...</td>\n",
       "      <td>t3_omv7p</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Keatonus</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101700</th>\n",
       "      <td>question</td>\n",
       "      <td>Did anyone else spot the Floatzen 2 DVD?\\nI kn...</td>\n",
       "      <td>t3_48toyo</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Pradfanne</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101701</th>\n",
       "      <td>answer</td>\n",
       "      <td>I didn't notice it until it was too late. What...</td>\n",
       "      <td>t1_d0mhap4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Portgas</td>\n",
       "      <td>t3_48toyo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101702</th>\n",
       "      <td>answer</td>\n",
       "      <td>I don't know, it was to quick.\\nI just liked t...</td>\n",
       "      <td>t1_d0n2wdx</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Pradfanne</td>\n",
       "      <td>t1_d0mhap4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101703</th>\n",
       "      <td>answer</td>\n",
       "      <td>The Frozen parody was on the right edge of the...</td>\n",
       "      <td>t1_d0msypp</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>SaucyServine</td>\n",
       "      <td>t1_d0mhap4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101704</th>\n",
       "      <td>elaboration</td>\n",
       "      <td>There's a bunch of references in that one scen...</td>\n",
       "      <td>t1_d0n92qp</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>thawed_caveman</td>\n",
       "      <td>t3_48toyo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101705 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                type                                               body  \\\n",
       "0       announcement  4/7/13  \\n\\n7/27/12  \\n\\nhttp://www.imdb.com/t...   \n",
       "1        elaboration  I've wanted to watch this for a long time. I w...   \n",
       "2        elaboration  You strike me as the type who would appreciate...   \n",
       "3        elaboration  Yeah, I've always heard that Altman was famous...   \n",
       "4       announcement  Alright guys, little background about myself. ...   \n",
       "...              ...                                                ...   \n",
       "101700      question  Did anyone else spot the Floatzen 2 DVD?\\nI kn...   \n",
       "101701        answer  I didn't notice it until it was too late. What...   \n",
       "101702        answer  I don't know, it was to quick.\\nI just liked t...   \n",
       "101703        answer  The Frozen parody was on the right edge of the...   \n",
       "101704   elaboration  There's a bunch of references in that one scen...   \n",
       "\n",
       "                id  is_first_post  depth          author    reply_to  \n",
       "0        t3_1bx6qw              1      0          DTX120        None  \n",
       "1       t1_c9b2nyd              0      1       mcgrewf10   t3_1bx6qw  \n",
       "2       t1_c9b30i1              0      2          DTX120  t1_c9b2nyd  \n",
       "3       t1_c9b6sj0              0      3       mcgrewf10  t1_c9b30i1  \n",
       "4         t3_omv7p              1      0        Keatonus        None  \n",
       "...            ...            ...    ...             ...         ...  \n",
       "101700   t3_48toyo              1      0       Pradfanne        None  \n",
       "101701  t1_d0mhap4              0      1         Portgas   t3_48toyo  \n",
       "101702  t1_d0n2wdx              0      2       Pradfanne  t1_d0mhap4  \n",
       "101703  t1_d0msypp              0      2    SaucyServine  t1_d0mhap4  \n",
       "101704  t1_d0n92qp              0      1  thawed_caveman   t3_48toyo  \n",
       "\n",
       "[101705 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate number of posts in thread and whetehr commenter is  OP\n",
    "\n",
    "post_count = []\n",
    "is_OP = []\n",
    "word_count = []\n",
    "sent_count = []\n",
    "parent_word_count = []\n",
    "parent_sent_count = []\n",
    "\n",
    "counter = 1\n",
    "for ix in range(0,len(data_orig)):\n",
    "    \n",
    "    if data_orig.loc[ix].is_first_post == 1:\n",
    "        post_count += counter*[counter]\n",
    "        counter = 1\n",
    "        \n",
    "        #checking wether the commenter is also the Original Poster\n",
    "        OP = data_orig.loc[ix].author\n",
    "        is_OP.append(1)\n",
    "        \n",
    "        parent_sent_count.append(0)\n",
    "        parent_word_count.append(0)\n",
    "        \n",
    "    else:\n",
    "        counter += 1\n",
    "        \n",
    "        #checking wether the commenter is also the Original Poster\n",
    "        if data_orig.loc[ix].author == OP:\n",
    "            is_OP.append(1)\n",
    "        else:\n",
    "            is_OP.append(0)\n",
    "        try:\n",
    "            parent_sent_count.append(sent_count[data_orig[data_orig.id == data_orig.loc[ix].reply_to].index[0]])\n",
    "            parent_word_count.append(word_count[data_orig[data_orig.id == data_orig.loc[ix].reply_to].index[0]])\n",
    "        except:\n",
    "            parent_sent_count.append(0)\n",
    "            parent_word_count.append(0)\n",
    "            #print('PARENT NOT FOUND')\n",
    "   \n",
    "     # Idk how to vectorize this, so i just do it here but slowly\n",
    "    words = word_tokenize(data_orig.loc[ix].body)\n",
    "    sents = sent_tokenize(data_orig.loc[ix].body)\n",
    "    word_count.append(len(words))\n",
    "    sent_count.append(len(sents))\n",
    "    \n",
    "\n",
    "post_count += counter*[counter]\n",
    "del post_count[0]\n",
    "\n",
    "data_orig['nr_of_posts'] = post_count\n",
    "data_orig['is_OP'] = is_OP\n",
    "data_orig['sent_count'] = sent_count\n",
    "data_orig['word_count'] = word_count\n",
    "data_orig['parent_sent_count'] = parent_sent_count\n",
    "data_orig['parent_word_count'] = parent_sent_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['parent_word_count',\n",
       " 'type',\n",
       " 'body',\n",
       " 'id',\n",
       " 'is_first_post',\n",
       " 'depth',\n",
       " 'author',\n",
       " 'reply_to',\n",
       " 'nr_of_posts',\n",
       " 'is_OP',\n",
       " 'sent_count',\n",
       " 'word_count',\n",
       " 'parent_sent_count']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = data_orig.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are manually selected obvious bot accounts with bot in their name\n",
    "bot_accounts = ['autowikibot', 'flair_your_post_bot', 'gracefulcharitybot', 'domoarigatobtfcboto', 'reddtipbot',\n",
    "               'Mariners_bot', 'rightsbot', 'ttumblrbots', 'image_linker_bot', 'conspirobot', 'dogetipbot', 'juicedb_bot',\n",
    "               'telltalebot', 'hearthscan-bot', 'gifv-bot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_orig[cols]\n",
    "for bot in range(0, len(bot_accounts)):\n",
    "    df = df[df['author'] != bot_accounts[bot]]\n",
    "df = df[df.body != '']\n",
    "df = df[df.body != '[deleted]']\n",
    "\n",
    "\n",
    "punct = '\\t\\n\\r'\n",
    "\n",
    "processed_posts = []\n",
    "\n",
    "\n",
    "for post in df.body:\n",
    "    post = post.translate(str.maketrans('', '', punct))\n",
    "    processed_posts.append(post)\n",
    "\n",
    "df.drop(columns=['body'])\n",
    "df['body'] = processed_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['normalized_depth'] = (df.depth)/(df.nr_of_posts)\n",
    "df['character_count'] = df.body.str.len()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['structure'] = np.column_stack((df.depth,df.character_count, df.word_count,df.normalized_depth,  df.sent_count, df.is_OP, df.parent_word_count, df.parent_sent_count)).tolist()\n",
    "\n",
    "\n",
    "#df['structure'] = np.column_stack((df.depth, df.normalized_depth, df.character_count, df.word_count,  df.sent_count)).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      98868\n",
       "unique        10\n",
       "top       answer\n",
       "freq       40153\n",
       "Name: type, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#df.loc[(df['type'] != 'question') & (df['type'] != 'answer'), 'type'] = 'other'\n",
    "df.type.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yeah, I've always heard that Altman was famous for his ensemble casts. But I, too, have never seen an Altman.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = df[0:4]\n",
    "test.body[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('../data/annotated_posts_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_json('../data/train.json', orient='records', lines = True)\n",
    "test.to_json('../data/test.json', orient='records', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate post depth \n",
    "\n",
    "'''\n",
    "data_orig['reply_to'] = data_orig.reply_to.fillna(\"root\")\n",
    "deepts = []\n",
    "exceptions = 0\n",
    "\n",
    "for ix in range(0, len(data_orig)):\n",
    "    depth = 0\n",
    "    \n",
    "    reply_to = data_orig.loc[ix].reply_to\n",
    "    ID = data_orig.loc[ix].id\n",
    "    \n",
    "    try:\n",
    "        while(reply_to != 'root'):\n",
    "            rt_ix = data_orig.index[data_orig['id'] == reply_to]\n",
    "            rt_ix = rt_ix[0]\n",
    "            reply_to = data_orig.loc[rt_ix].reply_to\n",
    "            depth += 1\n",
    "\n",
    "        deepts.append([depth])\n",
    "        \n",
    "    except:\n",
    "        exceptions += 1\n",
    "        deepts.append([0])\n",
    "\n",
    "print('exceptions', exceptions)\n",
    "print(deepts)\n",
    "\n",
    "data_orig['depth'] = deepts\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
